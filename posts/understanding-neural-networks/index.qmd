---
title: "Understanding Neural Networks: The Core of Modern AI"
author: "Lixuan Lin"
date: "2025-01-17"
categories: [Neural Networks, Machine Learning, AI]
image: "neural-network.png"
format: 
  html:
    page-layout: full
---


## Introduction

Neural networks are the backbone of modern artificial intelligence (AI), enabling breakthroughs in image recognition, natural language processing, and autonomous systems. But what exactly are neural networks, and how do they work? In this blog, we’ll break down the concept of neural networks in simple terms to help you understand their core principles and applications.

## What Are Neural Networks?

A neural network is a computational model inspired by the structure and functioning of the human brain. It consists of interconnected units called **neurons**, organized into layers:

![Layers in Neural Networks](layers.png)

- **Input Layer**: Receives the raw data (e.g., an image or a sentence).

- **Hidden Layers**: Perform computations by passing data through neurons using weights and biases.

- **Output Layer**: Produces the final result (e.g., classifying an image as a cat or dog).

Each connection between neurons has a **weight**, which determines the importance of the input, and a **bias**, which adjusts the output.

**Analogy**: Think of neural networks as a network of decision-makers, where each neuron contributes to the overall decision by processing small pieces of information.

## How Neural Networks Work?

The functioning of a neural network can be summarized in two key steps:

![Neural Network Workflow](workflow.png)

### a. Forward Propagation
Data flows through the network from the input layer to the output layer. Each neuron applies a mathematical operation to its input, producing an output.

**Activation functions** introduce non-linearity to the model, enabling it to solve complex problems. Common activation functions include:

![Activation Functions](activation-functions.ppm)

- **Sigmoid**: Converts inputs into values between 0 and 1.
- **ReLU** (Rectified Linear Unit): Outputs the input directly if positive, otherwise zero.
- **Tanh**: Maps inputs to values between -1 and 1.

### b. Backpropagation and Learning
After generating an output, the network calculates the error (difference between predicted and actual output). Using an optimization algorithm like **gradient descent**, it adjusts the weights and biases to minimize this error. This process is called **backpropagation**.

**Optimization algorithms** like **stochastic gradient descent (SGD)** or **Adam** are used to efficiently adjust the network’s parameters. These methods ensure faster and more accurate learning.

**Analogy**: Imagine teaching a child to throw a ball into a basket. Each failed attempt helps adjust their technique until they succeed. Similarly, backpropagation adjusts the network's parameters to improve its accuracy.

## Applications of Neural Networks

Neural networks have transformed industries by solving complex problems. Here are some real-world applications:

- **Image Recognition**: Used in medical diagnostics (e.g., identifying tumors in X-rays) and social media platforms (e.g., facial recognition).
- **Natural Language Processing (NLP)**: Powering chatbots, translation tools, and sentiment analysis.
- **Autonomous Vehicles**: Enabling self-driving cars to detect objects, understand traffic signs, and make decisions.
- **Recommendation Systems**: Suggesting products, movies, or songs based on user preferences (e.g., Netflix, Spotify).

## Advantages and Limitations

### a. Advantages
- **Flexibility**: Can model complex, non-linear relationships.
- **Adaptability**: Capable of learning from large, diverse datasets.
- **Wide Applications**: Useful across various domains like healthcare, finance, and entertainment.

### b. Limitations
- **Computational Cost**: Requires significant computational resources.
- **Data Dependence**: Needs large amounts of labeled data for training.
- **Interpretability**: Often described as a “black box,” making it hard to understand how decisions are made.

## Case Study: Handwritten Digit Recognition

One famous application of neural networks is recognizing handwritten digits using the MNIST dataset. The dataset consists of 70,000 images of handwritten digits (0-9). Neural networks process these images to classify the digits accurately.

![Handwritten Digit Recognition](handwritten.png)

Steps in the process:

- **Data Preprocessing**: Images are scaled to a uniform size and normalized.
- **Model Training**: A neural network learns from the labeled examples.
- **Prediction**: Given a new image, the network predicts the digit with high accuracy.

This application demonstrates the power of neural networks in real-world scenarios.

## Conclusion

Neural networks are powerful tools that have revolutionized AI, enabling machines to tackle tasks once thought impossible. By understanding their structure, functionality, and applications, we gain insight into the potential and limitations of this transformative technology.

As neural networks continue to evolve, their impact on industries and daily life will only grow. The journey of understanding these models is just the beginning of exploring the vast possibilities of AI.

The field of AI is rapidly advancing. Several exciting developments and trends are shaping the future of this technology: self-supervised learning, explainable AI (XAI), federated learning, neurosymbolic AI and so on. You will see introductions of them in the upcoming blogs!

## References

1. Ian Goodfellow, Yoshua Bengio, and Aaron Courville. *Deep Learning*. MIT Press, 2016.
2. Towards Data Science. "Understanding Activation Functions in Neural Networks".
3. Andrew Ng. "Introduction to Neural Networks." Coursera Machine Learning Course.
4. Mike Gelbart, Tomas Beuzen, Varada Kolhatkar, Prajeet Bajpai, Arman Seyed-Ahmadi, and Aaron Berk. (2025). *Supervised Learning II*. Available under the Attribution 4.0 International (CC BY 4.0). Retrieved from [this link](https://pages.github.ubc.ca/MDS-2024-25/DSCI_572_sup-learn-2_students/README.html).

